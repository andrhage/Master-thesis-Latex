\documentclass[USenglish]{ifimaster}  %% ... or USenglish or norsk or nynorsk
\usepackage[utf8]{inputenc}           %% ... or latin1
\usepackage[T1]{fontenc,url}
\usepackage{todonotes}
\urlstyle{sf}
\usepackage{babel,textcomp,csquotes,duomasterforside,varioref,graphicx}
\usepackage[backend=biber,style=numeric-comp]{biblatex}
\usepackage{acronym}
\usepackage{amsmath , amssymb , amsthm}

%\setcounter{tocdepth}{5}

\title{Image-based terrain characterization for autonomous vehicles, based on deep learning}        %% ... or whatever

\author{Andreas Hagen}                      %% ... or whoever 

\bibliography{references.bib}                  %% ... or whatever

\begin{document}
\duoforside[dept={Department of Technology Systems},   %% ... or your department
  program={Cybernetics},  %% ... or your programme
  short]                                        %% ... or long

\frontmatter{}
\chapter*{Abstract}                   %% ... or Sammendrag or Samandrag

\tableofcontents{}
\listoffigures{}
\listoftables{}

\chapter*{Preface}
\chapter*{Abbreviations}
%\addcontentsline{toc}{chapter}{Abbreviations} \noindent
%--- Acronyms -----------------------------------------------------------------%
% \acrodef{label}[acronym]{written out form} % acronym syntax
%\acrodef{etacar}[$\eta$ Car]{Eta Carinae}   % acronym example
%--- Acronyms -----------------------------------------------------------------%
% how to use acronyms:
% \ac = use acronym, first time write both, full name and acronym
% \acf = use full name (text + acronym)
% \acs = only use acronym
% \acl = only use long text
% \acp, acfp, acsp, aclp = use plural form for acronym (append 's')
% \acsu, aclu = write + mark as used
% \acfi = write full name in italics and acronym in normal style
% \acused = mark acronym as used
% \acfip = full, emphasized, plural, used
%--- Acronyms -----------------------------------------------------------------%
\begin{acronym}
        \acro{ai}[AI]{Artificial intelligence}
        \acro{ann}[ANN]{Artificial neural network}
        \acro{cnn}[CNN]{Convolutional neural network}
        \acro{dl}[DL]{Deep learning}
        \acro{ml}[ML]{Machine learning}
        \acro{cpu}[CPU]{Central processing unit}
        \acro{gpu}[GPU]{Graphics processing unit}
        \acro{rgb}[RGB]{Red,Green,Blue}
\end{acronym}


\mainmatter{}
\chapter{Introduction}                  
\section{Motivation}
\section{Background}
\section{Problem description}
\section{Thesis outline}

\chapter{Theoretical background}
This chapter will provide an overview of the basics of \ac{ai}, \ac{ml}, and \ac{dl}. These fields and techniques are the basis for this thesis. Then cover the necessary remaining theory and mathematics related to the thesis, and lastly go over related works. \todo{gj√∏r om etter kapittelet er ferdigskrevet}
\section{Artificial intelligence}
\ac{ai} is a field where machines are able to demonstrate intelligence through mathematics, statistics and logic. It has the ability to tackle many complex problems that are intellectually difficult or impossible to solve for a human being with natural intelligence. Even though \ac{ai} could solve complex problems, it still had a few challenges in the early days. Some intuitive tasks for us, like recognizing a cat or a dog in an image or the context of a written text, proved to be a true challenge. A solution to these problems was to allow machines to learn from experience, which is where \ac{ml} and \ac{dl} comes into the picture\cite{The_holy_grail_of_DL}.
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{bilder/AI_ML_DL.png}
    \caption{The history of AI, ML, and DL
    \protect\cite{website:AI}}
    \label{fig:AI}
\end{figure}
\subsection{Machine learning}
Figure \ref{fig:AI} illustrates that \ac{ml} is a subset from \ac{ai}. Instead of solve the task with explicit instructions, \ac{ml} learns from experience.
\todo{mere juice}
\subsection{Deep learning}
Instead of learning from specific tasks like in \ac{ml}, \ac{dl} learns data representations from data sets. Both \ac{ml} and \ac{dl} learns from unsupervised-, semi-supervised- or supervised learning. \todo{mere juice}
\subsubsection{Supervised learning}
Supervised learning is the most commonly used technique in \ac{dl} \cite{Francois_Deep_learning_with_python}. The word supervised refers to known targets or annotations in the form of a labelled data set. With that knowledge a function can learn how to map input data to the targets. \todo{sett inn formel og forklar}
\subsubsection{Keras}
Keras is a user-friendly \ac{dl} library developed in Python. It was originally made for researchers as way to do quick experimentation, with an easy implementation. Keras has quickly gained popularity among its users and is one of the most popular framework in \ac{dl} projects nowadays. The main reasons for the popularity is the user-friendliness and that Keras can run with the same code seamlessly on \ac{cpu} or \ac{gpu}\cite{Francois_Deep_learning_with_python}.
\subsection{Benefits using deep learning vs traditional methods}
\section{Computer vision}
Computer vision is a field which main purpose is to make machines be able to interpret and understand features from images or video. In other words, "Teaching computers how to see"\cite{website:maskinsyn-intro}. Computer vision has many sub categories. This section will go through the ones being used in the thesis.
\subsection{Semantic segmentation}
\subsection{Morphological operations}
A common binary image operations are called \textit{morphological operations}, since
they change the shape of the underlying binary objects \cite{Ritter}. This is done by convolving a binary \textit{structuring element} on the binary image, where the structuring element can be any chosen shape. These operations are typically used to clean up binary images, and two of the standard binary morphological operations used in our project are: 
\begin{itemize}
    \item \textbf{Erosion}
    \item \textbf{Dilation}
\end{itemize}

Where \textit{erosion} thins the object and \textit{dilation} thickens the object. As one may notice, there is some noise in the thresholded image in figure \todo{fig:seg}, which is a consequence of thresholding an image by color. Using these operations in this specific order (erosion + dilation) results in \textit{opening}. This operation tends to close large regions, smooth boundaries, while removing the noise (white dots) as shown in the figure \todo{fig:morph}. The program erodes twice to remove white dots, then do five dilations to fill the contours, making them more visible in the segmented image. The structuring element used is a 3x3 rectangular kernel.
\todo{skriv om binary dilations}
\subsection{Connected component analysis}

\section{Data preprocessing}
This section will cover data processing from an image-based point of view. Before the data set can be fed into the network, would it in the most cases be inevitable to perform several preprocesses. This is necessary in order to make it ready for training. 
\newline
A common thing to do with an image-based data set is to resize the images to a slightly lower resolution. This is especially necessary if the computer does not have a state-of-art \ac{gpu}, as the model otherwise might be very slow. The down scaling will help the model to be more effective and less time consuming, at the prize of loosing some features from the original resolution. It is therefore important to test different scales, to find the perfect fit between keeping enough key features and have an effective model.
\newline
A \ac{rgb} image contains integer values in the range from 0-255 in each of its three channels. Because the values of the weights in an \ac{ann} is relatively small, it is normal practice to normalize the image-array to values between 0-1. This can be done by divide the array with 255. Doing so will prevent to slow down the learning process, as the values from the weights and the array now are in closer range. Casting the array from int to float before normalizing would increase the accuracy even further. This is due to the float division results in a more accurate number than int division.\todo{legg til bilde eller ligning med forklaring} 
\newline
\newline
Another process is to check whether the data set has the correct shape or not. This is necessary because the network needs to know which input shape to expect. The input layer in Keras is a tensor which are passed to the first hidden layer. If that input layer does not correspond with the shape of each element in the data set, the network will not be able to execute. In the case where "Conv2D" layers are used in a Keras framework, an input array needs to have the following structure:
\newline
\newline
(height, width, channels)
\newline
\newline
Where height and width refers to the x and y coordinates in the image, and channels refers to if the image is binary (channels=1) or \ac{rgb} (channels=3).
\subsection{Data augmentation}
Data augmentation is a helpful tool in order to maintain a more generalized model. It is a method for applying transformations to the training data. When the images are pre-proccessed with the methods described in this section, the network learns how to cope with slightly different images than the original training set. This is the reason the model have a higher chance of predicting the test images (images that the model never have seen before) with more accuracy. In addition to potentially higher accuracy, the model also has lower chance of overfitting. This sub-section will only cover the most used augmentation techniques.  
\subsubsection{Random cropping}
A popular method in augmentation is random cropping. Which means sampling a random chosen square box from the original image, and then resize to the original size.\todo{sett inn bilde} As seen in figure \todo{referer til bildet} the image focus on different areas from the original image, due to the random chosen box. This operation must be included with the ground truth images. When an operation changes the geometry in the image, the same operation must be done in the ground truth image in order to still be a valid target. 
\subsubsection{Flipping images}
Another method is to flip the images in either vertical or horizontal order. Even when the images are flipped, they are still recognizable for the model. The ground truth must also go through the same operation. \todo{sett inn bilde av flipped}Figure \todo{referer til figur} illustrates images with targets being flipped both vertically and horizontally.  
\subsubsection{Color changes}
The idea behind color changes is to make the the model more robust and generalized for new unseen data. Since the geometry in the images are the same after applying color changes, there is not necessary to anything with the ground truth images.
\section{Training, validation and testing data}
\todo{dobbeltsjekk alt fra cs231n, skriv mer ryddig/forst√•elig}
In order to have a successful implementation, the data set must be split into different parts. Either splitting the data set in training and test, or training, validation and test. The network should only train on the training set, then check accuracy on the validate set and predict values on the testing set. The validation- and test set should be data that the network never has seen before. There is a rule of thumb to split the data in training, validation and test set 60/20/20.  
\todo{skriv om sklearn biblioteket(i metode kap), og hvorfor det er viktig √• splitte datasettet korrekt}
\newpage
\section{Artificial neural networks}
Mostly of the theory in this section is based on the theory from the Stanford University course \textit{"CS231n: Convolutional Neural Networks for Visual Recognition"}\cite{website:cs231n}, and from the course \textit{"INF4490 ‚Äì Biologically Inspired Computing"}\cite{website:inf_4490_slp}\cite{website:inf_4490_mlp} at University of Oslo.
\newline
\newline
An \ac{ann} is a computing system which is vaguely based on the same principle as biological neurons in a human brain. It has the ability to learn different tasks and data representations. To fully understand the concept, the basics of a neural network is divided into different parts and further described.
\newline
\newline
\subsection{Single- and multi-layer neural network}
In 1943 McCulloch \& Pitts designed a much simplified version of biological neurons\cite{mcculloch_pitts}. With their design, they are widely known as the inventors of the first \ac{ann}. Their ideas of a threshold in the activation function and combining many basic units in order to increase computational power are still being used nowadays.
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{bilder/mcculloch_and_pitts.png}
    \caption{Illustrates the McCulloch \& Pitts design of a simplified neuron \cite{website:mcCulloch_img}}
    \label{fig:mcculoch_and_pitts}
\end{figure}
The illustration of the neuron and its activation function in figure \ref{fig:mcculoch_and_pitts} can be mathematically explained with equation \ref{eq:mcCulloch}.
\begin{equation}\label{eq:mcCulloch}
\begin{aligned}
    {h = \sum_{i=1}^{n} x_i \omega_i \quad , \quad\quad o = 
\begin{cases}
    1 & \text{ h $\geq$ $\theta$ }  \\
    0 & \text{ h < $\theta$ }
\end{cases}}
\end{aligned}
\end{equation}
Where the neurons function ($h$) is denoted in the form of a dot product between the inputs ($x_i$) and the weights ($\omega_i$). The neurons activation function \textit{"fires"} when the dot product of the input and its weight respectively are higher than a given threshold $\theta$. Meaning the output ($o$) becomes 1 when $h$ is equal to or higher than the threshold, and 0 when $h$ is lower than the threshold value.
\newline
\newline
If many McCulloch \& Pitts neurons are put together, the structure of a single-layer neural network appears. A single-layer perceptron like that are able to learn linear problems. When the task is to learn non-linear problems, the solution is to add one or more hidden layers as done in multi-layer perceptron.   
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{bilder/slp_&_mlp.jpeg}
    \caption{Single-layer perceptron to the left, and multi-layer perceptron to the right \cite{website:slp_mlp}}
    \label{fig:slp_mlp}
\end{figure}
The difference can be seen visually by figure \ref{fig:slp_mlp}, where the single-layer perceptron only has an input and an output layer, while the multi-layer perceptron includes at least one hidden layer.
\newline
\newline
\subsection{The learning rule}
In order for the single-layer neural network to learn, it has to adjust the weights accordingly. This is were the perceptron learning rule comes into the picture.

\begin{equation}\label{eq:learning_rule}
\begin{aligned}
\omega_{ij} \longleftarrow \omega_{ij} + \Delta\omega_{ij}
\end{aligned}
\end{equation}
Equation \ref{eq:learning_rule} shows how the weight ($\omega_{ij}$) updates. The goal of the learning rule is to minimize the error at the output, such that $\Delta\omega_{ij} = 0 $. When the weights reach that state, they are tuned correctly. The weights can be both positive and negative, and how they adjust can be explained with the next equation. 

\begin{equation}\label{eq:delta_learning_rule}
\begin{aligned}
\Delta\omega_{ij} = \eta * (t_j - y_j) * x_i 
\end{aligned}
\end{equation}

In equation \ref{eq:delta_learning_rule} $\eta$ is referred to as the learning rate. It is a scalar which decide how much the weight value in each iteration will change. Finding the right balance in the choice of learning rate is therefore crucial. A high value (e.g. 1) can create an unstable net. With a high learning rate will the weights change a lot every time they updates. %The reason is that the weight will change a lot every time it updates with a high learning rate. 
Choosing a low value will make a stable network, but will require much more learning time, because the weights uses more time to tune into correct values.
\newline
\newline
$\eta$ is further being multiplied with the error ($(t_j - y_j)$, where $t_j$ is the target output and $y_j$ is the actual output). Before finally being multiplied with the inputs ($x_i$). As stated above, the goal is to minimize this error. Which is done during training where the weights are adjusted with equation \ref{eq:delta_learning_rule}.
\subsection{Bias}
In the case where all inputs are zero, the weights will have no effect since they are multiplied with the inputs. The solution for that particular case is to have an adjustable threshold, which can be applied with a bias node. The bias node should be added to each neuron. Then equation \ref{eq:mcCulloch} will become equation \ref{eq:bias}, where $b$ is the bias.
\begin{equation}\label{eq:bias}
\begin{aligned}
    {h = \sum_{i=1}^{n} x_i \omega_i + b \quad , \quad\quad o = 
\begin{cases}
    1 & \text{ h $\geq$ $\theta$ }  \\
    0 & \text{ h < $\theta$ }
\end{cases}}
\end{aligned}
\end{equation}
\subsection{Learning in multi-layer neural networks}
When learning in a single-layer neural network, there is possible to gain knowledge over which weight is correct or not, as described with the learning rule. In multi-layer perceptron there is at least one hidden layer between the input and the output. Hence, it is impossible to know which weights are correct, and which activations being correct for the neurons in the hidden layer. Without knowing which weight or activation is correct, there is not possible to learn the weights or train the network. The problem of not being able to train a multi-layer neural network was solved in 1986 with an algorithm called backpropagation \cite{Rumelhart:1986:LIR:104279.104293}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{bilder/forward_prop.png}
    \caption{The forward pass in backpropagation \cite{website:inf_4490_mlp}}
    \label{fig:forward_step}
\end{figure}

The backpropagation algorithm consists of two main steps. The first being the forward pass, which has the following structure illustrated in figure \ref{fig:forward_step}. After the input layer has received its inputs, the activations of the hidden nodes in the middle layer is calculated. Lastly the activations of the output nodes in the last layer is being calculated. 

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{bilder/backward_prop.png}
    \caption{The backward pass in backpropagation \cite{website:inf_4490_mlp}}
    \label{fig:backward_step}
\end{figure}

The second step in the backpropagation is called the backward pass, and is illustrated in figure \ref{fig:backward_step}. This step starts by calculating the output errors in the last layer, before it update the same layers weights. Then the error is being propagated backwards, and the hidden weights in the middle layer is updated. This process is repeated until the first layer is reached.
\subsection{Gradient descent learning and momentum}
When training the network with backpropagation, the goal is to minimizing the errors in the network. As described with the backward pass, after being calculated, the errors from the output layer are propagated backwards in the network. The tool being used is a form of gradient descent. 

\begin{equation}\label{eq:sum_of_error}
\begin{aligned}
E(w) = \frac{1}{2} \sum_{k}(t_k - y_k)^2 = \frac{1}{2}\sum_{k}(t_k - \sum_{i} \omega_{ik}x_i)^2
\end{aligned}
\end{equation}

It differentiate the sum-of squares error in equation \ref{eq:sum_of_error}, showed with the equation \ref{eq:gradient_descent}.  

\begin{equation}\label{eq:gradient_descent}
\begin{aligned}
\Delta\omega_{ik} = -\eta\frac{\delta E}{\delta \omega_{ik}}
\end{aligned}
\end{equation}

Even though gradient descent algorithm is a good method for finding the minimum value, it has a potential risk of getting stuck in a local minimum as visualized in figure \ref{fig:gradient_descent}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{bilder/gradient_descent.png}
    \caption{Gradient descent \cite{website:inf_4490_mlp}}
    \label{fig:gradient_descent}
\end{figure}

There is two alternatives to avoid being stuck in a local minimum. The first one is to initialize the training several times with random weights. The other method is to use momentum. If the gradient descent algorithm reaches a local minimum, the momentum keeps the algorithm going further uphill for a while, until the descending starts again and hopefully a global minimum will be found instead. Momentum is described mathematically in equation \ref{eq:momentum}. 

\begin{equation}\label{eq:momentum}
\begin{aligned}
w_{ij} \longleftarrow w_{ij} - \eta\Delta_j z_i+\alpha\Delta w^{t-1}_{ij}
\end{aligned}
\end{equation}

\subsection{Activation functions}
To decide if a neuron should \textit{"fire"} or not, an activation function is used. In other words, the activation function takes a number and performs a mathematically operation on it. There exist several different activations functions, and the one being used in this thesis will be covered here.
\subsubsection{The sigmoid function}
The sigmoid function is a popular activation function.

\begin{equation}\label{eq:sigmoid}
\begin{aligned}
\sigma(x) = \frac{1}{(1 + e^{-x})}
\end{aligned}
\end{equation}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{bilder/sigmoid_function.png}
    \caption{Sigmoid activation \cite{website:cs231n_activation_functions}}
    \label{fig:sigmoid}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{bilder/relu_function.png}
    \caption{Relu activation \cite{website:cs231n_activation_functions}}
    \label{fig:relu}
\end{figure}

\subsection{Loss}
\subsection{Optimizers}
\subsection{Regularization}
\subsection{Predictions}
After the network has updated the internal state well enough and learned the input and output, it can be fed new data it has never seen before. It will predict an output given the input of the new data, given the networks internal state.  
\subsection{Convolutional neural network}
A \ac{cnn} is a sub category of \ac{ann}. The main difference between a \ac{cnn} and an ordinary neural network is how the input is interpret. \todo{snakk om at cnn bruker f√¶rre parametre pga konvolusjon enn et ordin√¶rt nett, f.eks fcn}
\subsubsection{History of CNNs}
\subsubsection{Convolution}
\subsubsection{Layers}
\subsubsection{U-net}
\section{Active learning}
\subsection{Annotating the data set}
When the training set needs to labeled, there is quite a few options to choose a suitable program. Most of them is online driven, meaning the data set needs to be uploaded to a website, and the annotation can further be done from there. A program which offer offline annotation is called \textit{"labelme"}\cite{website:labelme}. 
\section{Related works}
\newpage
\chapter{Method}
\section{Active learning}
\subsection{Annotation the training set}
\subsection{Choosing the right images for further training}
\subsection{Train again with auto-labeled data}
\section{Tensorboard}
A handy way to gain control over the training, in order to tweak parameters and control the models accuracy, is using Tensorboard. Tensorboard is a visual tool developed by TensorFlow providing full overview of the training, while training. It is easy to set up in the terminal.
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{bilder/tensorboard_anaconda_prompt.PNG}
    \caption{Initializing Tensorboard from terminal}
    \label{fig:tensorboard_anaconda_prompt}
\end{figure}
\newline
As illustrated in figure \ref{fig:tensorboard_anaconda_prompt} Tensorboard is up and running with the command \textit{< tensorboard --logdir=<log> >}. 
\newline
When using Tensorboard with Keras, there is necessary to use callbacks in the code. As stated by the Keras documentation on their website.
\newline
\newline
\textit{"A callback is a set of functions to be applied at given stages of the training procedure. You can use callbacks to get a view on internal states and statistics of the model during training. You can pass a list of callbacks (as the keyword argument callbacks) to the .fit() method of the Sequential or Model classes. The relevant methods of the callbacks will then be called at each stage of the training."}\cite{website:Keras_doc}. 
\newline
\newline
It is therefore possible with callbacks to decide by own specifications what to include and when to gain information during the training process.  
\begin{figure}[ht]
    \centering
    \includegraphics[width=1\textwidth]{bilder/tensorboard_acc_loss.PNG}
    \caption{Accuracy and loss for both training and validation visualized in Tensorboard}
    \label{fig:accuracy_loss}
\end{figure}
\newline
Two popular graphs used to check the quality of the \ac{dl} model is \textit{"accuracy"} and \textit{"loss"}, which are found under the section \textit{"SCALARS"}. 
\begin{figure}[ht]
    \centering
    \includegraphics[width=1\textwidth]{bilder/overfitting_underfitting.png}
    \caption{Underfitting, perfect sampling and overfitting \cite{website:overfitting_underfitting}}
    \label{fig:overfitting_underfitting}
\end{figure}
To have the opportunity to visually see these graphs during training, is an advantage. This is due to the possibility of easily spotting errors like \textit{"overfitting"} or \textit{"underfitting"}, and if the accuracy in the model is fulfilling user expectations. Examples of underfitting from the left image, perfect sampling in the middle and overfitting in the right image, can be seen in figure \ref{fig:overfitting_underfitting}. As seen in figure \ref{fig:accuracy_loss} the accuracy and loss in both training and validation fulfil expectations. 
By observing that the loss function steadily decreases into a smooth curve indicates that there is neither \textit{"overfitting"} nor \textit{"underfitting"} in the model.

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.8\textwidth]{bilder/tensorboard_graph.PNG}
    \caption{The model graph visualized in Tensorboard}
    \label{fig:graph}
\end{figure}
The other section in Tensorboard called \textit{"GRAPHS"} shows the model graph. Being able to see the layers, optimizer function and connections, helps the user see the whole model in a better perspective. There is possible to rename each layer in the model according to the user's own request. 
\section{Predictions}
\subsection{Thresholding}


\chapter{Results}                     

\chapter{Discussion}



\backmatter{}
\printbibliography
\end{document}
